![alt text](../../../media/image-foundrylocal4.png)

Before you begin, ensure you have completed the steps in the [Get Started with Foundry Local](lab-get-started-with-foundry-local.md) guide.

## Introduction
Using Foundry Local + the OpenAI SDK combines local sovereignty/performance with a mature, widely adopted developer interface.

Key benefits:

- **Single API surface**: Same chat/completions interface works locally and (later) against Azure or OpenAI-hosted models with minimal changes (just base_url + key).
- **Faster iteration**: Local model startup + no network latency speeds prompt and agent loop tuning.
- **Privacy & sovereignty**: Data stays on the machine for sensitive prototyping; no accidental external calls.
- **Seamless fallback**: Easy to implement: try local first, fallback to a remote (Azure) deployment when the task exceeds local capability.
- **Ecosystem compatibility**: Works with existing OpenAI SDK middleware (logging, retry, tracing wrappers).
- **Feature parity path**: When local adds support for features (streaming, function/tool calling), existing SDK patterns transfer directly.
- **Model abstraction**: Use aliases (e.g., phi-3.5-mini) while deferring exact model selection/updates to Foundry Local.
- **Cost control**: Free/local experimentation before moving higher-quality or larger context workloads to cloud models.
- **Offline resilience**: Enables partially disconnected agent workflows.
- **Testing & CI**: Deterministic/local test harnesses without burning cloud quota.
- **Progressive enhancement**: Start local â†’ selectively route queries (e.g., long context, higher reasoning) to cloud.

## Prerequisites
Install the following python packages if you haven't already:

```bash
pip install openai
pip install foundry-local-sdk
```

> TIP: If you are using a virtual environment, make sure to activate it before installing the packages! Use venv as conda library is blocked on some corporate machines. (command to use venv: `python -m venv .venv` and then `source .venv/bin/activate` on Mac/Linux or `.venv\Scripts\activate` on Windows)

## Use OpenAI SDK with Foundry Local

The following example demonstrates how to use the OpenAI SDK with Foundry Local. The code initializes the Foundry Local service, loads a model, and generates a response using the OpenAI SDK.

Copy-and-paste the following code into a Python file named app.py:

```python
import openai
from foundry_local import FoundryLocalManager

# By using an alias, the most suitable model will be downloaded 
# to your end-user's device. 
# alias = "phi-3.5-mini"
alias = "mistral-7b-v0.2"

# Create a FoundryLocalManager instance. This will start the Foundry
# Local service if it is not already running and load the specified model.
manager = FoundryLocalManager(alias)
# The remaining code uses the OpenAI Python SDK to interact with the local model.
# Configure the client to use the local Foundry service
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # API key is not required for local usage
)
# Set the model to use and generate a response
response = client.chat.completions.create(
    model=manager.get_model_info(alias).id,
    messages=[{"role": "user", "content": "I'm a business analyst working for Capgemini. I'm planning to upskill myself in AI and GenAI. Can you explain how my work will change over de next few years?"}]
)
print(response.choices[0].message.content)
```

Run the script using the command:

```bash
python3 app.py
```
You should see a response generated by the local model:

![alt text](../../../media/image-foundrylocal3.png)